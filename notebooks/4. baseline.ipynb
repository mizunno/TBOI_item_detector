{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import itertools\n",
    "import random\n",
    "import uuid\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "NUM_CLASSES = 716\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "DATA_PATH = \"../data/derived_data/data_augmented/\"\n",
    "RESULTS_PATH = \"./hps_search_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps_grid = dict(\n",
    "    conv2d_1_filters = [16],\n",
    "    conv2d_1_kernel = [3],\n",
    "    conv2d_2_filters = [32],\n",
    "    conv2d_2_kernel = [3],\n",
    "    conv2d_3_filters = [64],\n",
    "    conv2d_3_kernel = [3],\n",
    "    dense_1_units = [128],\n",
    "    pooling = [layers.MaxPooling2D],\n",
    "    optimizer = [\n",
    "        tf.keras.optimizers.Adam,\n",
    "    ],\n",
    "    learning_rate = [0.001]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(itertools.product(*hps_grid.values()))\n",
    "hps_combs = [dict(zip(hps_grid.keys(),params_sample)) for params_sample in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(hps_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(hps_comb):\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        layers.Conv2D(hps_comb['conv2d_1_filters'], hps_comb['conv2d_1_kernel'], padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(hps_comb['conv2d_2_filters'], hps_comb['conv2d_2_kernel'], padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(hps_comb['conv2d_3_filters'], hps_comb['conv2d_3_kernel'], padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(hps_comb['dense_1_units'], activation='relu'),\n",
    "        layers.Dense(NUM_CLASSES)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hps_comb['optimizer'](learning_rate=hps_comb['learning_rate']),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        test_ds,\n",
    "        checkpoint_filepath=None,\n",
    "        save_model_path=None,\n",
    "        epochs=15\n",
    "    ):\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #     filepath=checkpoint_filepath,\n",
    "    #     save_weights_only=True,\n",
    "    #     monitor='val_accuracy',\n",
    "    #     save_best_only=True\n",
    "    # )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            # early_stopping,\n",
    "            # model_checkpoint,\n",
    "        ],\n",
    "        verbose='auto',\n",
    "    )\n",
    "    \n",
    "    if save_model_path:\n",
    "        model.save(save_model_path)\n",
    "    \n",
    "    results = history.history\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "    results['test_loss'] = test_loss\n",
    "    results['test_acc'] = test_acc\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_hps_search(train_ds, val_ds, test_ds, output_path):\n",
    "    hps_search_id = uuid.uuid4()\n",
    "\n",
    "    output_path = f'{output_path}/{hps_search_id}/'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for hps_comb in tqdm.tqdm(hps_combs):\n",
    "        print(hps_comb)\n",
    "        hps_comb_id = uuid.uuid4()\n",
    "\n",
    "        model = define_model(hps_comb)\n",
    "\n",
    "        results = train_model(\n",
    "            model, \n",
    "            train_ds=train_ds,\n",
    "            val_ds=val_ds,\n",
    "            test_ds=test_ds,\n",
    "            save_model_path=f'{output_path}/{hps_comb_id}.keras'\n",
    "        )\n",
    "\n",
    "        for k,v in hps_comb.items():\n",
    "            results[k] = str(v)\n",
    "\n",
    "        with open(f'{output_path}/{hps_comb_id}.json', 'w') as f:\n",
    "            json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH,\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH,\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take(val_batches // 2)\n",
    "val_ds = val_ds.skip(val_batches // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hps_search(train_ds, val_ds, test_ds, RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = tf.keras.models.load_model('hps_search_results/2da1ec8e-7a2f-4150-80bc-c4745fdb8c86/a01928f8-7a00-4e7a-a249-c0edacefecf4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval model for sanity checking\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model predictions on test set\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model outputs (logits) have to be passed through the softmax function to normalize them to a probability distribution\n",
    "y_pred = tf.nn.softmax(y_pred)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class assigned to each sample is the position of the max value\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classes from the Dataset object\n",
    "y_true = [y for x, y in test_ds]\n",
    "y_true = (np.array(y_true)).flatten()\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Confusion Matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (35.0, 35.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Implementing visualization of Confusion Matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m[:50,:50], display_labels=labels[:50])\n",
    "display_c_m.plot(cmap='OrRd', xticks_rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
