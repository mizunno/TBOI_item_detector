{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import itertools\n",
    "import random\n",
    "import uuid\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "NUM_CLASSES = 716\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "DATA_PATH = \"../../data/derived_data/data_augmented/\"\n",
    "RESULTS_PATH = \"./hps_search_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps_grid = dict(\n",
    "    conv2d_1_filters = [8, 16, 32, 64],\n",
    "    conv2d_1_kernel = [3, 5],\n",
    "    conv2d_2_filters = [8, 16, 32, 64],\n",
    "    conv2d_2_kernel = [3, 5],\n",
    "    conv2d_3_filters = [8, 16, 32, 64],\n",
    "    conv2d_3_kernel = [3, 5],\n",
    "    dense_1_units = [64, 128, 256],\n",
    "    pooling = [layers.MaxPooling2D, layers.AveragePooling2D],\n",
    "    optimizer = [\n",
    "        tf.keras.optimizers.Adam,\n",
    "        tf.keras.optimizers.RMSprop,\n",
    "        tf.keras.optimizers.SGD\n",
    "    ],\n",
    "    learning_rate = [0.01, 0.001, 0.0001]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(itertools.product(*hps_grid.values()))\n",
    "hps_combs = [dict(zip(hps_grid.keys(),params_sample)) for params_sample in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(hps_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(hps_comb):\n",
    "\n",
    "    pooling = hps_comb['pooling']\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        layers.Conv2D(hps_comb['conv2d_1_filters'], hps_comb['conv2d_1_kernel'], padding='same', activation='relu'),\n",
    "        pooling(),\n",
    "        layers.Conv2D(hps_comb['conv2d_2_filters'], hps_comb['conv2d_2_kernel'], padding='same', activation='relu'),\n",
    "        pooling(),\n",
    "        layers.Conv2D(hps_comb['conv2d_3_filters'], hps_comb['conv2d_3_kernel'], padding='same', activation='relu'),\n",
    "        pooling(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(hps_comb['dense_1_units'], activation='relu'),\n",
    "        layers.Dense(NUM_CLASSES)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hps_comb['optimizer'](learning_rate=hps_comb['learning_rate']),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        test_ds,\n",
    "        checkpoint_filepath=None, \n",
    "        epochs=1000\n",
    "    ):\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #     filepath=checkpoint_filepath,\n",
    "    #     save_weights_only=True,\n",
    "    #     monitor='val_accuracy',\n",
    "    #     save_best_only=True\n",
    "    # )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            early_stopping,\n",
    "            # model_checkpoint,\n",
    "        ],\n",
    "        verbose='auto',\n",
    "    )\n",
    "    \n",
    "    results = history.history\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "    results['test_loss'] = test_loss\n",
    "    results['test_acc'] = test_acc\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_hps_search(train_ds, val_ds, test_ds, output_path):\n",
    "    hps_search_id = uuid.uuid4()\n",
    "\n",
    "    output_path = f'{output_path}/{hps_search_id}/'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for hps_comb in tqdm.tqdm(hps_combs):\n",
    "        hps_comb_id = uuid.uuid4()\n",
    "\n",
    "        model = define_model(hps_comb)\n",
    "\n",
    "        results = train_model(\n",
    "            model, \n",
    "            train_ds=train_ds,\n",
    "            val_ds=val_ds,\n",
    "            test_ds=test_ds,\n",
    "        )\n",
    "\n",
    "        for k,v in hps_comb:\n",
    "            results[k] = v\n",
    "\n",
    "        with open(f'{output_path}/{hps_comb_id}.json', 'w') as f:\n",
    "            json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH,\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH,\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take(val_batches // 2)\n",
    "val_ds = val_ds.skip(val_batches // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hps_search(train_ds, val_ds, test_ds, RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
